{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeartDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)       \n",
    "    def __len__(self):\n",
    "        return len(self.X)   \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class DeepHeartNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepHeartNet, self).__init__()       \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),            \n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class HeartDiseasePredictor:\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        self.create_directories()\n",
    "        \n",
    "    def create_directories(self):\n",
    "        self.dirs = {\n",
    "            'models': os.path.join(self.save_path, 'models'),\n",
    "            'plots': os.path.join(self.save_path, 'plots'),\n",
    "            'results': os.path.join(self.save_path, 'results')\n",
    "        }\n",
    "        for dir_path in self.dirs.values():\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    def prepare_data(self, data_path):\n",
    "        print(f\"Loading data from {data_path}\")\n",
    "        data = pd.read_csv(data_path)\n",
    "        \n",
    "        print(\"\\nDataset Info:\")\n",
    "        print(data.info())\n",
    "        print(\"\\nSample Data:\")\n",
    "        print(data.head())\n",
    "        \n",
    "        with open(os.path.join(self.dirs['results'], 'data_info.txt'), 'w') as f:\n",
    "            f.write(\"Dataset Shape: \" + str(data.shape) + \"\\n\")\n",
    "            f.write(\"\\nColumns: \" + str(data.columns.tolist()) + \"\\n\")\n",
    "            data.info(buf=f)\n",
    "            f.write(\"\\nDescriptive Statistics:\\n\")\n",
    "            data.describe().to_string(buf=f)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def train_model(self, X_train, X_test, y_train, y_test):\n",
    "        print(\"Preparing training data...\")\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "        print(f\"y_train shape: {y_train.shape}\")\n",
    "        print(f\"y_test shape: {y_test.shape}\")\n",
    "        \n",
    "        train_dataset = HeartDataset(X_train, y_train)\n",
    "        test_dataset = HeartDataset(X_test, y_test)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "        \n",
    "        input_dim = X_train.shape[1]\n",
    "        model = DeepHeartNet(input_dim).to(self.device)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "        )\n",
    "        \n",
    "        n_epochs = 200\n",
    "        best_loss = float('inf')\n",
    "        patience = 20\n",
    "        no_improve = 0\n",
    "        history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "        \n",
    "        print(\"\\nStarting training...\")\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                batch_y = batch_y.to(self.device).view(-1, 1)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in test_loader:\n",
    "                    batch_X = batch_X.to(self.device)\n",
    "                    batch_y = batch_y.to(self.device).view(-1, 1)\n",
    "                    \n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    predicted = (outputs > 0.5).float()\n",
    "                    total += batch_y.size(0)\n",
    "                    correct += (predicted == batch_y).sum().item()\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(test_loader)\n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_acc'].append(accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, '\n",
    "                      f'Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "            \n",
    "            scheduler.step(avg_val_loss)            \n",
    "            if avg_val_loss < best_loss:\n",
    "                best_loss = avg_val_loss\n",
    "                torch.save(model.state_dict(), \n",
    "                         os.path.join(self.dirs['models'], 'best_model.pth'))\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "        \n",
    "        self._save_training_history(history)\n",
    "        \n",
    "        model.load_state_dict(torch.load(os.path.join(self.dirs['models'], 'best_model.pth')))\n",
    "        self._evaluate_model(model, test_loader, y_test)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _save_training_history(self, history):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirs['plots'], 'training_history.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        pd.DataFrame(history).to_csv(\n",
    "            os.path.join(self.dirs['results'], 'training_history.csv'), \n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "    def _evaluate_model(self, model, test_loader, y_test):\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, _ in test_loader:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                outputs = model(batch_X)\n",
    "                probs = outputs.cpu().numpy()\n",
    "                preds = (outputs > 0.5).float().cpu().numpy()\n",
    "                all_probs.extend(probs)\n",
    "                all_preds.extend(preds)\n",
    "        \n",
    "        all_preds = np.array(all_preds).flatten()\n",
    "        all_probs = np.array(all_probs).flatten()\n",
    "        \n",
    "        report = classification_report(y_test, all_preds)\n",
    "        with open(os.path.join(self.dirs['results'], 'classification_report.txt'), 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, all_preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.savefig(os.path.join(self.dirs['plots'], 'confusion_matrix.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_test, all_probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC)')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(os.path.join(self.dirs['plots'], 'roc_curve.png'))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data from /project/zhiwei/hf78/ecg/data/heart/heart.csv\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1025 entries, 0 to 1024\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       1025 non-null   int64  \n",
      " 1   sex       1025 non-null   int64  \n",
      " 2   cp        1025 non-null   int64  \n",
      " 3   trestbps  1025 non-null   int64  \n",
      " 4   chol      1025 non-null   int64  \n",
      " 5   fbs       1025 non-null   int64  \n",
      " 6   restecg   1025 non-null   int64  \n",
      " 7   thalach   1025 non-null   int64  \n",
      " 8   exang     1025 non-null   int64  \n",
      " 9   oldpeak   1025 non-null   float64\n",
      " 10  slope     1025 non-null   int64  \n",
      " 11  ca        1025 non-null   int64  \n",
      " 12  thal      1025 non-null   int64  \n",
      " 13  target    1025 non-null   int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 112.2 KB\n",
      "None\n",
      "\n",
      "Sample Data:\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
      "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
      "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
      "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
      "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   2     3       0  \n",
      "1   0     3       0  \n",
      "2   0     3       0  \n",
      "3   1     3       0  \n",
      "4   3     2       0  \n",
      "Preparing training data...\n",
      "X_train shape: (820, 13)\n",
      "X_test shape: (205, 13)\n",
      "y_train shape: (820,)\n",
      "y_test shape: (205,)\n",
      "\n",
      "Starting training...\n",
      "Epoch 0: Train Loss: 0.4896, Val Loss: 0.3672, Accuracy: 82.93%\n",
      "Epoch 10: Train Loss: 0.1880, Val Loss: 0.1395, Accuracy: 96.59%\n",
      "Epoch 20: Train Loss: 0.1251, Val Loss: 0.0669, Accuracy: 97.07%\n",
      "Epoch 30: Train Loss: 0.0971, Val Loss: 0.0352, Accuracy: 100.00%\n",
      "Epoch 40: Train Loss: 0.0797, Val Loss: 0.0158, Accuracy: 100.00%\n",
      "Epoch 50: Train Loss: 0.0800, Val Loss: 0.0080, Accuracy: 100.00%\n",
      "Epoch 00058: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 60: Train Loss: 0.0371, Val Loss: 0.0063, Accuracy: 100.00%\n",
      "Epoch 70: Train Loss: 0.0270, Val Loss: 0.0083, Accuracy: 100.00%\n",
      "Epoch 00079: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 80: Train Loss: 0.0298, Val Loss: 0.0040, Accuracy: 100.00%\n",
      "Epoch 90: Train Loss: 0.0378, Val Loss: 0.0031, Accuracy: 100.00%\n",
      "Epoch 100: Train Loss: 0.0440, Val Loss: 0.0037, Accuracy: 100.00%\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 110: Train Loss: 0.0278, Val Loss: 0.0023, Accuracy: 100.00%\n",
      "Epoch 00113: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 00119: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 120: Train Loss: 0.0254, Val Loss: 0.0023, Accuracy: 100.00%\n",
      "Epoch 00125: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 130: Train Loss: 0.0138, Val Loss: 0.0023, Accuracy: 100.00%\n",
      "Epoch 00132: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 140: Train Loss: 0.0322, Val Loss: 0.0020, Accuracy: 100.00%\n",
      "Epoch 00141: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 00147: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 150: Train Loss: 0.0420, Val Loss: 0.0016, Accuracy: 100.00%\n",
      "Epoch 00153: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Early stopping triggered\n",
      "\n",
      "All results have been saved to /project/zhiwei/hf78/ecg/output/heart\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    save_path = '/project/zhiwei/hf78/ecg/output/heart'    \n",
    "    predictor = HeartDiseasePredictor(save_path)\n",
    "    \n",
    "    data = predictor.prepare_data('/project/zhiwei/hf78/ecg/data/heart/heart.csv')    \n",
    "    X = data.drop('target', axis=1) \n",
    "    y = data['target']    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    predictor.train_model(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "    \n",
    "    print(f\"\\nAll results have been saved to {save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
